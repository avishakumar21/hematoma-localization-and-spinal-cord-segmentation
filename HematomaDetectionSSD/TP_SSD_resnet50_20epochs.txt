PS C:\Users\akumar80\Documents\Avisha Kumar Lab Work\hematoma localization\HematomaDetectionSSD> & C:/Users/akumar80/Documents/Software/envs/snakesDL/python.exe "c:/Users/akumar80/Documents/Avisha Kumar Lab Work/hematoma localization/HematomaDetectionSSD/train.py"
Number of training samples: 236
Number of validation samples: 83

C:\Users\akumar80\Documents\Software\envs\snakesDL\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be 
removed in the future, please use 'weights' instead.
  warnings.warn(
C:\Users\akumar80\Documents\Software\envs\snakesDL\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are 
deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
SSD(
  (backbone): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)   
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (5): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (6): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (7): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (anchor_generator): DefaultBoxGenerator(aspect_ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]], clip=True, scales=[0.15, 0.3, 0.44999999999999996, 0.6, 0.75, 0.9, 1.0], steps=None)
  (head): SSDHead(
    (classification_head): SSDClassificationHead(
      (module_list): ModuleList(
        (0): Conv2d(2048, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): Conv2d(1024, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): Conv2d(512, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (4-5): 2 x Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (regression_head): SSDRegressionHead(
      (module_list): ModuleList(
        (0): Conv2d(2048, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (4-5): 2 x Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (transform): GeneralizedRCNNTransform(
      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
      Resize(min_size=(640,), max_size=640, mode='bilinear')
  )
)
24,641,780 total parameters.
24,641,780 training parameters.
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 1 of 20
Training
Loss: 4.2036: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:35<00:00,  2.39s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.88s/it]
Epoch #1 train loss: 6.523
Epoch #1 mAP@0.50:0.95: 0.17793111503124237
Epoch #1 mAP@0.50: 0.5709218978881836
Took 1.021 minutes for epoch 0

BEST VALIDATION mAP: 0.17793111503124237

SAVING BEST MODEL FOR EPOCH: 1

SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 2 of 20
Training
Loss: 3.0381: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.20s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.94s/it]
Epoch #2 train loss: 3.467
Epoch #2 mAP@0.50:0.95: 0.30743902921676636
Epoch #2 mAP@0.50: 0.8258444666862488
Took 0.974 minutes for epoch 1

BEST VALIDATION mAP: 0.30743902921676636

SAVING BEST MODEL FOR EPOCH: 2

SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 3 of 20
Training
Loss: 2.5130: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.23s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.99s/it]
Epoch #3 train loss: 2.616
Epoch #3 mAP@0.50:0.95: 0.5014545321464539
Epoch #3 mAP@0.50: 0.9889150857925415
Took 0.978 minutes for epoch 2

BEST VALIDATION mAP: 0.5014545321464539

SAVING BEST MODEL FOR EPOCH: 3

SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 4 of 20
Training
Loss: 1.5356: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.21s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.92s/it]
Epoch #4 train loss: 1.926
Epoch #4 mAP@0.50:0.95: 0.5988711714744568
Epoch #4 mAP@0.50: 0.9886683225631714
Took 0.961 minutes for epoch 3

BEST VALIDATION mAP: 0.5988711714744568

SAVING BEST MODEL FOR EPOCH: 4

SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 5 of 20
Training
Loss: 1.3836: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.22s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:24<00:00,  4.00s/it]
Epoch #5 train loss: 1.466
Epoch #5 mAP@0.50:0.95: 0.6017667651176453
Epoch #5 mAP@0.50: 0.9985905885696411
Took 0.973 minutes for epoch 4

BEST VALIDATION mAP: 0.6017667651176453

SAVING BEST MODEL FOR EPOCH: 5

SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 6 of 20
Training
Loss: 1.2212: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:36<00:00,  2.46s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.89s/it]
Epoch #6 train loss: 1.138
Epoch #6 mAP@0.50:0.95: 0.6751039028167725
Epoch #6 mAP@0.50: 0.9983071684837341
Took 1.021 minutes for epoch 5

BEST VALIDATION mAP: 0.6751039028167725

SAVING BEST MODEL FOR EPOCH: 6

SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 7 of 20
Training
Loss: 0.8571: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.24s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.90s/it]
Epoch #7 train loss: 1.029
Epoch #7 mAP@0.50:0.95: 0.6958981156349182
Epoch #7 mAP@0.50: 0.9982591271400452
Took 0.969 minutes for epoch 6

BEST VALIDATION mAP: 0.6958981156349182

SAVING BEST MODEL FOR EPOCH: 7

SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 8 of 20
Training
Loss: 0.7896: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.24s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.98s/it]
Epoch #8 train loss: 0.852
Epoch #8 mAP@0.50:0.95: 0.7093169093132019
Epoch #8 mAP@0.50: 0.9993093013763428
Took 0.976 minutes for epoch 7

BEST VALIDATION mAP: 0.7093169093132019

SAVING BEST MODEL FOR EPOCH: 8

SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 9 of 20
Training
Loss: 0.9476: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.26s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.97s/it]
Epoch #9 train loss: 0.758
Epoch #9 mAP@0.50:0.95: 0.6794676184654236
Epoch #9 mAP@0.50: 0.9801980257034302
Took 0.979 minutes for epoch 8
SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 10 of 20
Training
Loss: 0.4581: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.22s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:24<00:00,  4.01s/it]
Epoch #10 train loss: 0.747
Epoch #10 mAP@0.50:0.95: 0.6986499428749084
Epoch #10 mAP@0.50: 0.9801980257034302
Took 0.974 minutes for epoch 9
SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 11 of 20
Training
Loss: 0.4544: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.22s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.85s/it]
Epoch #11 train loss: 0.650
Epoch #11 mAP@0.50:0.95: 0.7157726883888245
Epoch #11 mAP@0.50: 0.9801980257034302
Took 0.958 minutes for epoch 10

BEST VALIDATION mAP: 0.7157726883888245

SAVING BEST MODEL FOR EPOCH: 11

SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 12 of 20
Training
Loss: 0.5827: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.21s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.89s/it]
Epoch #12 train loss: 0.607
Epoch #12 mAP@0.50:0.95: 0.7197456955909729
Epoch #12 mAP@0.50: 0.9801980257034302
Took 0.959 minutes for epoch 11

BEST VALIDATION mAP: 0.7197456955909729

SAVING BEST MODEL FOR EPOCH: 12

SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 13 of 20
Training
Loss: 0.4816: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.25s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.92s/it]
Epoch #13 train loss: 0.619
Epoch #13 mAP@0.50:0.95: 0.71270352602005
Epoch #13 mAP@0.50: 0.9995340704917908
Took 0.970 minutes for epoch 12
SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 14 of 20
Training
Loss: 0.4427: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.25s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.98s/it]
Epoch #14 train loss: 0.527
Epoch #14 mAP@0.50:0.95: 0.7161785960197449
Epoch #14 mAP@0.50: 0.9801980257034302
Took 0.978 minutes for epoch 13
SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 15 of 20
Training
Loss: 0.4768: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.25s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.98s/it]
Epoch #15 train loss: 0.506
Epoch #15 mAP@0.50:0.95: 0.727146565914154
Epoch #15 mAP@0.50: 0.9993093013763428
Took 0.978 minutes for epoch 14

BEST VALIDATION mAP: 0.727146565914154

SAVING BEST MODEL FOR EPOCH: 15

SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 16 of 20
Training
Loss: 0.6328: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.24s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.90s/it]
Epoch #16 train loss: 0.513
Epoch #16 mAP@0.50:0.95: 0.7195338010787964
Epoch #16 mAP@0.50: 0.9801980257034302
Took 0.968 minutes for epoch 15
SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 17 of 20
Training
Loss: 0.5832: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.24s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.89s/it]
Epoch #17 train loss: 0.453
Epoch #17 mAP@0.50:0.95: 0.731898844242096
Epoch #17 mAP@0.50: 0.9997642636299133
Took 0.968 minutes for epoch 16

BEST VALIDATION mAP: 0.731898844242096

SAVING BEST MODEL FOR EPOCH: 17

SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 18 of 20
Training
Loss: 0.3805: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.23s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.93s/it]
Epoch #18 train loss: 0.440
Epoch #18 mAP@0.50:0.95: 0.7398194074630737
Epoch #18 mAP@0.50: 0.9997642636299133
Took 0.967 minutes for epoch 17

BEST VALIDATION mAP: 0.7398194074630737

SAVING BEST MODEL FOR EPOCH: 18

SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 19 of 20
Training
Loss: 0.4711: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:32<00:00,  2.17s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.92s/it]
Epoch #19 train loss: 0.445
Epoch #19 mAP@0.50:0.95: 0.7418730854988098
Epoch #19 mAP@0.50: 0.9997642636299133
Took 0.952 minutes for epoch 18

BEST VALIDATION mAP: 0.7418730854988098

SAVING BEST MODEL FOR EPOCH: 19

SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.

EPOCH 20 of 20
Training
Loss: 0.4934: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:32<00:00,  2.18s/it] 
Validating
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.83s/it]
Epoch #20 train loss: 0.410
Epoch #20 mAP@0.50:0.95: 0.7585449814796448
Epoch #20 mAP@0.50: 0.9997642636299133
Took 0.946 minutes for epoch 19

BEST VALIDATION mAP: 0.7585449814796448

SAVING BEST MODEL FOR EPOCH: 20

SAVING PLOTS COMPLETE...
Adjusting learning rate of group 0 to 5.0000e-04.